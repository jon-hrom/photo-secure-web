import json
import os
import requests
import re
from urllib.parse import urlparse, parse_qs, urljoin, quote
import tempfile
import boto3
from datetime import datetime
import psycopg2
from psycopg2.extras import RealDictCursor
from PIL import Image
from io import BytesIO
from bs4 import BeautifulSoup

def handler(event: dict, context) -> dict:
    '''API –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–æ—Ç–æ –ø–æ URL (–Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫, Google Drive –∏ –¥—Ä.)'''
    method = event.get('httpMethod', 'GET')
    
    if method == 'OPTIONS':
        return {
            'statusCode': 200,
            'headers': {
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'POST, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type, X-User-Id'
            },
            'body': ''
        }
    
    if method != 'POST':
        return {
            'statusCode': 405,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({'error': 'Method not allowed'})
        }
    
    # –ü–æ–ª—É—á–∞–µ–º user_id –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    user_id = event.get('headers', {}).get('X-User-Id', '')
    if not user_id:
        return {
            'statusCode': 401,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({'error': 'Unauthorized'})
        }
    
    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
    conn = psycopg2.connect(os.environ['DATABASE_URL'])
    cursor = conn.cursor(cursor_factory=RealDictCursor)
    
    # –ü–∞—Ä—Å–∏–º —Ç–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞
    try:
        body = json.loads(event.get('body', '{}'))
    except:
        cursor.close()
        conn.close()
        return {
            'statusCode': 400,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({'error': 'Invalid JSON'})
        }
    
    url = body.get('url', '').strip()
    folder_id = body.get('folder_id')
    
    if not url:
        cursor.close()
        conn.close()
        return {
            'statusCode': 400,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({'error': 'URL is required'})
        }
    
    # –ï—Å–ª–∏ folder_id –Ω–µ —É–∫–∞–∑–∞–Ω, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –ø–∞–ø–∫—É —Å –¥–∞—Ç–æ–π –∏ –≤—Ä–µ–º–µ–Ω–µ–º
    if not folder_id:
        folder_name = datetime.now().strftime('–ó–∞–≥—Ä—É–∑–∫–∞ %d.%m.%Y %H:%M')
        s3_prefix = f'uploads/{user_id}/{int(datetime.now().timestamp())}/'
        
        cursor.execute(
            '''INSERT INTO t_p28211681_photo_secure_web.photo_folders
               (user_id, folder_name, s3_prefix, folder_type, created_at, updated_at)
               VALUES (%s, %s, %s, %s, NOW(), NOW())
               RETURNING id''',
            (user_id, folder_name, s3_prefix, 'originals')
        )
        folder_id = cursor.fetchone()['id']
        conn.commit()
        print(f'[URL_UPLOAD] Created new folder: {folder_name} (id={folder_id})')
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø URL –∏ –ø–æ–ª—É—á–∞–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
    try:
        download_urls = get_download_urls(url)
    except Exception as e:
        cursor.close()
        conn.close()
        return {
            'statusCode': 400,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({'error': f'–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Å—ã–ª–∫—É: {str(e)}'})
        }
    
    if not download_urls:
        cursor.close()
        conn.close()
        return {
            'statusCode': 400,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({'error': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ñ–∞–π–ª—ã –ø–æ —Å—Å—ã–ª–∫–µ'})
        }
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.raw', '.cr2', '.nef', '.arw', '.dng'}
    filtered_urls = []
    
    for url_info in download_urls:
        name = url_info.get('name', '').lower()
        if any(name.endswith(ext) for ext in image_extensions):
            filtered_urls.append(url_info)
    
    if not filtered_urls:
        cursor.close()
        conn.close()
        return {
            'statusCode': 400,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({'error': '–ü–æ —Å—Å—ã–ª–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–æ—Ç–æ'})
        }
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 —Ñ–∞–π–ª–æ–≤ –∑–∞ —Ä–∞–∑ (—á—Ç–æ–±—ã —É–ª–æ–∂–∏—Ç—å—Å—è –≤ 30 —Å–µ–∫)
    max_files = 5
    total_found = len(filtered_urls)
    filtered_urls = filtered_urls[:max_files]
    
    print(f'[URL_UPLOAD] Found {total_found} images, will process first {len(filtered_urls)}')
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ S3
    s3 = boto3.client('s3',
        endpoint_url='https://bucket.poehali.dev',
        aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],
        aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY']
    )
    bucket = 'files'
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã
    uploaded_files = []
    failed_files = []
    
    for idx, url_info in enumerate(filtered_urls):
        try:
            download_url = url_info['url']
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            filename = url_info['name'].strip()
            
            print(f'[URL_UPLOAD] Processing {idx+1}/{len(filtered_urls)}: {filename}')
            
            # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª (—Å–Ω–∏–∂–∞–µ–º timeout)
            print(f'[URL_UPLOAD] üì• Downloading {filename} from {download_url[:100]}...')
            response = requests.get(download_url, timeout=8, stream=True)
            response.raise_for_status()
            
            print(f'[URL_UPLOAD] ‚úÖ Downloaded {filename}, size: {response.headers.get("content-length", "unknown")}')
            
            file_size = int(response.headers.get('content-length', 0))
            file_content = response.content
            
            # –ü–æ–ª—É—á–∞–µ–º folder s3_prefix –∏–∑ –ë–î
            cursor.execute(
                '''SELECT s3_prefix FROM t_p28211681_photo_secure_web.photo_folders
                   WHERE id = %s''',
                (folder_id,)
            )
            folder_result = cursor.fetchone()
            s3_prefix = folder_result['s3_prefix'] if folder_result else f'uploads/{user_id}/{int(datetime.now().timestamp())}/'
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ S3 —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∏–º–µ–Ω–µ–º
            s3_key = f'{s3_prefix}{filename}'
            
            print(f'[URL_UPLOAD] üì§ Uploading to S3: {s3_key}')
            
            s3.put_object(
                Bucket=bucket,
                Key=s3_key,
                Body=file_content,
                ContentType=response.headers.get('content-type', 'application/octet-stream')
            )
            
            print(f'[URL_UPLOAD] ‚úÖ Uploaded to S3 successfully')
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º CDN URL —Å –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤ (—Å–∫–æ–±–∫–∏, –ø—Ä–æ–±–µ–ª—ã –∏ —Ç.–¥.)
            aws_key_id = os.environ['AWS_ACCESS_KEY_ID']
            encoded_s3_key = quote(s3_key, safe='/')
            s3_url = f'https://cdn.poehali.dev/projects/{aws_key_id}/bucket/{encoded_s3_key}'
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–≤—å—é –¥–ª—è –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–∫—Ä–æ–º–µ RAW)
            thumbnail_s3_key = None
            thumbnail_s3_url = None
            width = None
            height = None
            is_raw = filename.lower().endswith(('.cr2', '.nef', '.arw', '.dng', '.raw'))
            
            # –î–ª—è RAW —Ñ–∞–π–ª–æ–≤ –ø—Ä–µ–≤—å—é –Ω–µ —Å–æ–∑–¥–∞—ë–º (—Ç—Ä–µ–±—É–µ—Ç—Å—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
            if not is_raw:
                try:
                    img = Image.open(BytesIO(file_content))
                    width, height = img.size
                    
                    print(f'[URL_UPLOAD] Image dimensions: {width}x{height}, size: {file_size} bytes')
                    
                    # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä thumbnail –¥–ª—è —á—ë—Ç–∫–æ—Å—Ç–∏ –ª–∏—Ü (1200px –≤–º–µ—Å—Ç–æ 800px)
                    max_thumb_size = 1200
                    img.thumbnail((max_thumb_size, max_thumb_size), Image.Resampling.LANCZOS)
                    
                    print(f'[URL_UPLOAD] Thumbnail size: {img.size}, max: {max_thumb_size}px')
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ JPEG –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    if img.mode in ('RGBA', 'LA', 'P'):
                        background = Image.new('RGB', img.size, (255, 255, 255))
                        if img.mode == 'P':
                            img = img.convert('RGBA')
                        background.paste(img, mask=img.getchannel('A') if 'A' in img.getbands() else None)
                        img = background
                    elif img.mode != 'RGB':
                        img = img.convert('RGB')
                    
                    # –ü–æ–≤—ã—à–µ–Ω–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è —á—ë—Ç–∫–æ—Å—Ç–∏ –¥–µ—Ç–∞–ª–µ–π (85 –≤–º–µ—Å—Ç–æ 75)
                    quality = 85
                    thumb_buffer = BytesIO()
                    img.save(thumb_buffer, format='JPEG', quality=quality, optimize=True)
                    thumb_buffer.seek(0)
                    
                    thumb_size = len(thumb_buffer.getvalue())
                    print(f'[URL_UPLOAD] Thumbnail buffer size: {thumb_size} bytes (quality={quality})')
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–≤—å—é –≤ S3
                    # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏–∑ filename, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å .jpg.jpg
                    base_name = os.path.splitext(filename)[0]
                    thumbnail_s3_key = f'{s3_prefix}thumbnails/{base_name}.jpg'
                    s3.put_object(
                        Bucket=bucket,
                        Key=thumbnail_s3_key,
                        Body=thumb_buffer.getvalue(),
                        ContentType='image/jpeg'
                    )
                    # CDN URL —Å –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
                    encoded_thumbnail_key = quote(thumbnail_s3_key, safe='/')
                    thumbnail_s3_url = f'https://cdn.poehali.dev/projects/{aws_key_id}/bucket/{encoded_thumbnail_key}'
                    
                    print(f'[URL_UPLOAD] ‚úÖ Generated thumbnail: {thumbnail_s3_key}')
                except Exception as thumb_error:
                    print(f'[URL_UPLOAD] ‚ö†Ô∏è Could not generate thumbnail: {str(thumb_error)}')
                    import traceback
                    print(f'[URL_UPLOAD] Traceback: {traceback.format_exc()}')
            else:
                print(f'[URL_UPLOAD] ‚è≠Ô∏è Skipping thumbnail for RAW file: {filename}')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
            print(f'[URL_UPLOAD] üì¶ Saving to DB: user_id={user_id}, folder_id={folder_id}, file_size={file_size}, width={width}, height={height}, has_thumbnail={thumbnail_s3_url is not None}')
            cursor.execute(
                '''INSERT INTO t_p28211681_photo_secure_web.photo_bank 
                   (user_id, folder_id, file_name, s3_key, s3_url, file_size, width, height, thumbnail_s3_key, thumbnail_s3_url, is_raw)
                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                   RETURNING id''',
                (user_id, folder_id, filename, s3_key, s3_url, file_size, width, height, thumbnail_s3_key, thumbnail_s3_url, is_raw)
            )
            photo_id = cursor.fetchone()['id']
            conn.commit()
            print(f'[URL_UPLOAD] ‚úÖ Committed to DB, photo_id={photo_id}')
            
            uploaded_files.append({
                'id': photo_id,
                'filename': filename,
                'size': file_size,
                's3_url': s3_url,
                'thumbnail_s3_url': thumbnail_s3_url
            })
            
            print(f'[URL_UPLOAD] ‚úÖ COMPLETE: {filename} (id={photo_id})')
            
        except Exception as e:
            print(f'[URL_UPLOAD] ‚ùå ERROR processing {url_info["name"]}: {str(e)}')
            import traceback
            print(f'[URL_UPLOAD] Traceback: {traceback.format_exc()}')
            failed_files.append({
                'filename': url_info['name'],
                'error': str(e)
            })
    
    cursor.close()
    conn.close()
    
    print(f'[URL_UPLOAD] üèÅ FINISHED: uploaded={len(uploaded_files)}, failed={len(failed_files)}, total_found={total_found}')
    
    return {
        'statusCode': 200,
        'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
        'body': json.dumps({
            'success': True,
            'total_found': total_found,
            'uploaded': len(uploaded_files),
            'failed': len(failed_files),
            'files': uploaded_files,
            'errors': failed_files,
            'folder_id': folder_id,
            'message': f'–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(uploaded_files)} –∏–∑ {total_found} —Ñ–æ—Ç–æ' if total_found > max_files else None
        })
    }


def get_download_urls(url: str) -> list:
    '''–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤'''
    
    # –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫
    if 'disk.yandex' in url or 'yadi.sk' in url:
        return get_yandex_disk_urls(url)
    
    # Google Drive
    elif 'drive.google.com' in url:
        return get_google_drive_urls(url)
    
    # Dropbox
    elif 'dropbox.com' in url:
        return get_dropbox_urls(url)
    
    # OneDrive / SharePoint
    elif '1drv.ms' in url or 'onedrive.live.com' in url or 'sharepoint.com' in url:
        return get_onedrive_urls(url)
    
    # Wfolio –∏–ª–∏ –¥—Ä—É–≥–∏–µ HTML-–≥–∞–ª–µ—Ä–µ–∏
    elif 'wfolio.ru' in url or url.endswith('/photos') or '/disk/' in url:
        return get_html_gallery_urls(url)
    
    # –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª
    else:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü–µ–π –∏–ª–∏ –ø—Ä—è–º–æ–π —Å—Å—ã–ª–∫–æ–π –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        parsed = urlparse(url)
        path_lower = parsed.path.lower()
        image_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.raw', '.cr2', '.nef', '.arw', '.dng', '.webp')
        
        if path_lower.endswith(image_extensions):
            # –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            filename = os.path.basename(parsed.path) or 'file.jpg'
            return [{'url': url, 'name': filename}]
        else:
            # –í–æ–∑–º–æ–∂–Ω–æ HTML-–≥–∞–ª–µ—Ä–µ—è - –ø—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å
            return get_html_gallery_urls(url)


def get_yandex_disk_urls(public_url: str) -> list:
    '''–ü–æ–ª—É—á–∞–µ—Ç —Ñ–∞–π–ª—ã —Å –Ø–Ω–¥–µ–∫—Å –î–∏—Å–∫–∞'''
    
    # API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø—É–±–ª–∏—á–Ω–æ–≥–æ —Ä–µ—Å—É—Ä—Å–∞
    api_url = 'https://cloud-api.yandex.net/v1/disk/public/resources'
    
    response = requests.get(api_url, params={'public_key': public_url, 'limit': 1000})
    response.raise_for_status()
    
    data = response.json()
    
    files = []
    
    # –ï—Å–ª–∏ —ç—Ç–æ —Ñ–∞–π–ª
    if data.get('type') == 'file':
        download_url = data.get('file')
        if download_url:
            files.append({
                'url': download_url,
                'name': data.get('name', 'file.jpg')
            })
    
    # –ï—Å–ª–∏ —ç—Ç–æ –ø–∞–ø–∫–∞
    elif data.get('type') == 'dir':
        items = data.get('_embedded', {}).get('items', [])
        for item in items:
            if item.get('type') == 'file':
                download_url = item.get('file')
                if download_url:
                    files.append({
                        'url': download_url,
                        'name': item.get('name', 'file.jpg')
                    })
    
    return files


def get_google_drive_urls(url: str) -> list:
    '''–ü–æ–ª—É—á–∞–µ—Ç —Ñ–∞–π–ª—ã —Å Google Drive'''
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º ID —Ñ–∞–π–ª–∞ –∏–ª–∏ –ø–∞–ø–∫–∏ –∏–∑ URL
    file_id_match = re.search(r'/d/([a-zA-Z0-9_-]+)', url) or re.search(r'id=([a-zA-Z0-9_-]+)', url)
    
    if not file_id_match:
        raise ValueError('–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å ID –∏–∑ —Å—Å—ã–ª–∫–∏ Google Drive')
    
    file_id = file_id_match.group(1)
    
    # –î–ª—è Google Drive –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'
    
    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–º—è —Ñ–∞–π–ª–∞
    try:
        head_response = requests.head(download_url, allow_redirects=True, timeout=10)
        content_disposition = head_response.headers.get('content-disposition', '')
        filename_match = re.search(r'filename="?([^"]+)"?', content_disposition)
        filename = filename_match.group(1) if filename_match else f'{file_id}.jpg'
    except:
        filename = f'{file_id}.jpg'
    
    return [{'url': download_url, 'name': filename}]


def get_dropbox_urls(url: str) -> list:
    '''–ü–æ–ª—É—á–∞–µ—Ç —Ñ–∞–π–ª—ã —Å Dropbox'''
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Å—ã–ª–∫—É –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤ —Å—Å—ã–ª–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    # www.dropbox.com/s/xxx/file.jpg?dl=0 ‚Üí dl.dropboxusercontent.com/s/xxx/file.jpg
    # www.dropbox.com/sh/xxx ‚Üí –ø–∞–ø–∫–∞ (—Ç—Ä–µ–±—É–µ—Ç API)
    
    files = []
    
    # –ï—Å–ª–∏ —ç—Ç–æ –ø—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ñ–∞–π–ª
    if '/s/' in url or '/scl/fi/' in url:
        # –ó–∞–º–µ–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä dl=0 –Ω–∞ dl=1 –¥–ª—è –ø—Ä—è–º–æ–≥–æ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        download_url = url.replace('dl=0', 'dl=1').replace('www.dropbox.com', 'dl.dropboxusercontent.com')
        
        # –ï—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ dl –Ω–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º
        if 'dl=' not in download_url:
            separator = '&' if '?' in download_url else '?'
            download_url = f"{download_url}{separator}dl=1"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL
        path_parts = urlparse(url).path.split('/')
        filename = path_parts[-1].split('?')[0] if path_parts else 'file.jpg'
        
        files.append({
            'url': download_url,
            'name': filename
        })
    
    # –ü–∞–ø–∫–∏ Dropbox –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è –±–µ–∑ API —Ç–æ–∫–µ–Ω–∞
    elif '/sh/' in url:
        raise ValueError('–°—Å—ã–ª–∫–∏ –Ω–∞ –ø–∞–ø–∫–∏ Dropbox –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ —Ñ–∞–π–ª.')
    
    return files


def get_onedrive_urls(url: str) -> list:
    '''–ü–æ–ª—É—á–∞–µ—Ç —Ñ–∞–π–ª—ã —Å OneDrive'''
    
    files = []
    
    # OneDrive –∏–º–µ–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Å—Å—ã–ª–æ–∫:
    # 1drv.ms/i/xxx (–∫–æ—Ä–æ—Ç–∫–∞—è —Å—Å—ã–ª–∫–∞)
    # onedrive.live.com/redir?resid=xxx
    # onedrive.live.com/embed?resid=xxx
    
    # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Å—ã–ª–æ–∫ (1drv.ms) - –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É
    if '1drv.ms' in url:
        try:
            # –°–ª–µ–¥—É–µ–º –∑–∞ —Ä–µ–¥–∏—Ä–µ–∫—Ç–æ–º
            response = requests.head(url, allow_redirects=True, timeout=10)
            final_url = response.url
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Å—ã–ª–∫—É –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
            if 'onedrive.live.com' in final_url:
                # –ó–∞–º–µ–Ω—è–µ–º embed/view –Ω–∞ download
                download_url = final_url.replace('/embed?', '/download?').replace('/view.aspx?', '/download.aspx?')
                
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–º—è —Ñ–∞–π–ª–∞
                filename_match = re.search(r'resid=([^&]+)', final_url)
                filename = filename_match.group(1) + '.jpg' if filename_match else 'file.jpg'
                
                files.append({
                    'url': download_url,
                    'name': filename
                })
            else:
                raise ValueError('–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Å—ã–ª–∫—É OneDrive')
        except Exception as e:
            raise ValueError(f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Å—ã–ª–∫–∏ OneDrive: {str(e)}')
    
    # –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ onedrive.live.com –∏–ª–∏ sharepoint.com
    elif 'onedrive.live.com' in url or 'sharepoint.com' in url:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ download URL
        download_url = url.replace('/embed?', '/download?').replace('/view.aspx?', '/download.aspx?')
        
        # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ download URL, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if '/download' not in download_url and '?download=1' not in download_url:
            separator = '&' if '?' in download_url else '?'
            download_url = f"{download_url}{separator}download=1"
        
        filename = 'file.jpg'  # OneDrive –Ω–µ –≤—Å–µ–≥–¥–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–º—è –≤ URL
        
        files.append({
            'url': download_url,
            'name': filename
        })
    
    return files


def get_html_gallery_urls(url: str) -> list:
    '''–ü–∞—Ä—Å–∏—Ç HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è'''
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ wfolio –ª–∏ —ç—Ç–æ
        if 'wfolio.ru' in url or '/disk/' in url:
            return get_wfolio_gallery_urls(url)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        # –ü–∞—Ä—Å–∏–º HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        base_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}"
        
        files = []
        seen_urls = set()
        
        # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
        # 1. –û–±—ã—á–Ω—ã–µ <img> —Ç–µ–≥–∏
        for img in soup.find_all('img'):
            src = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if src:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                full_url = urljoin(url, src)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∫–æ–Ω–∫–∏ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                if any(skip in full_url.lower() for skip in ['icon', 'logo', 'avatar', 'sprite']):
                    continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞
                filename = os.path.basename(urlparse(full_url).path.split('?')[0])
                
                # –î–æ–±–∞–≤–ª—è–µ–º –µ—Å–ª–∏ –µ—â–µ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ
                if full_url not in seen_urls and filename:
                    seen_urls.add(full_url)
                    files.append({
                        'url': full_url,
                        'name': filename or f'image_{len(files)+1}.jpg'
                    })
        
        # 2. –ò—â–µ–º –≤ srcset –∞—Ç—Ä–∏–±—É—Ç–∞—Ö (–¥–ª—è responsive –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
        for img in soup.find_all(['img', 'source']):
            srcset = img.get('srcset')
            if srcset:
                # srcset —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ø–∏—Å–æ–∫ URL —Å —Ä–∞–∑–º–µ—Ä–∞–º–∏: "url1 1x, url2 2x"
                for src_entry in srcset.split(','):
                    src = src_entry.strip().split()[0]
                    full_url = urljoin(url, src)
                    
                    if full_url not in seen_urls:
                        filename = os.path.basename(urlparse(full_url).path.split('?')[0])
                        if filename:
                            seen_urls.add(full_url)
                            files.append({
                                'url': full_url,
                                'name': filename or f'image_{len(files)+1}.jpg'
                            })
        
        # 3. –ò—â–µ–º —Ñ–æ–Ω–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ style –∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        for element in soup.find_all(style=True):
            style = element.get('style', '')
            bg_matches = re.findall(r'url\(["\']?([^"\')]+)["\']?\)', style)
            for bg_url in bg_matches:
                full_url = urljoin(url, bg_url)
                
                if full_url not in seen_urls:
                    filename = os.path.basename(urlparse(full_url).path.split('?')[0])
                    if filename and any(full_url.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                        seen_urls.add(full_url)
                        files.append({
                            'url': full_url,
                            'name': filename or f'image_{len(files)+1}.jpg'
                        })
        
        # 4. –ò—â–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è wfolio
        for picture in soup.find_all('picture'):
            for source in picture.find_all('source'):
                src = source.get('srcset', '').split()[0] if source.get('srcset') else None
                if src:
                    full_url = urljoin(url, src)
                    if full_url not in seen_urls:
                        filename = os.path.basename(urlparse(full_url).path.split('?')[0])
                        if filename:
                            seen_urls.add(full_url)
                            files.append({
                                'url': full_url,
                                'name': filename or f'image_{len(files)+1}.jpg'
                            })
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–∏—Å–∫–ª—é—á–∞–µ–º SVG, –∏–∫–æ–Ω–∫–∏ –∏ —Ç.–¥.)
        image_extensions = ('.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.webp', '.raw', '.cr2', '.nef', '.arw', '.dng')
        filtered_files = []
        
        for file_info in files:
            filename_lower = file_info['name'].lower()
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏ —Ä–∞–∑–º–µ—Ä URL (–æ—Ç—Å–µ–∫–∞–µ–º base64)
            if any(filename_lower.endswith(ext) for ext in image_extensions):
                if not file_info['url'].startswith('data:'):
                    filtered_files.append(file_info)
        
        print(f'[HTML_GALLERY] Found {len(filtered_files)} images on page')
        
        return filtered_files
    
    except Exception as e:
        print(f'[HTML_GALLERY] Error parsing page: {str(e)}')
        raise ValueError(f'–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É: {str(e)}')


def get_wfolio_gallery_urls(url: str) -> list:
    '''–ü–æ–ª—É—á–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –≥–∞–ª–µ—Ä–µ–∏ wfolio'''
    
    try:
        # –ü–∞—Ä—Å–∏–º URL —á—Ç–æ–±—ã –ø–æ—Å—Ç—Ä–æ–∏—Ç—å API endpoint
        parsed = urlparse(url)
        
        # URL —Ñ–æ—Ä–º–∞—Ç–∞: https://ponomarev-pro.ru/disk/ds-vishenka-3-gruppa-z1pqps/photos
        # API endpoint: /disk/ds-vishenka-3-gruppa-z1pqps/pieces?design_variant=masonry&folder_path=photos
        
        path_parts = parsed.path.strip('/').split('/')
        
        if len(path_parts) < 2:
            raise ValueError('–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL wfolio –≥–∞–ª–µ—Ä–µ–∏')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º disk_id –∏ folder_path
        disk_id = path_parts[1] if len(path_parts) > 1 else None
        folder_path = path_parts[2] if len(path_parts) > 2 else 'photos'
        
        if not disk_id:
            raise ValueError('–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å ID –≥–∞–ª–µ—Ä–µ–∏ –∏–∑ URL')
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º API endpoint
        api_url = f"{parsed.scheme}://{parsed.netloc}/disk/{disk_id}/pieces"
        params = {
            'design_variant': 'masonry',
            'folder_path': folder_path
        }
        
        print(f'[WFOLIO] Fetching gallery from: {api_url}')
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml',
            'Referer': url
        }
        
        response = requests.get(api_url, params=params, headers=headers, timeout=15)
        response.raise_for_status()
        
        # –ü–∞—Ä—Å–∏–º HTML –æ—Ç–≤–µ—Ç
        soup = BeautifulSoup(response.text, 'html.parser')
        
        files = []
        seen_urls = set()
        
        # –ò—â–µ–º –≤—Å–µ —Ñ–æ—Ç–æ-—ç–ª–µ–º–µ–Ω—Ç—ã –≤ –≥–∞–ª–µ—Ä–µ–µ
        # wfolio –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É: <div class="piece" data-piece-id="...">
        pieces = soup.find_all('div', class_='piece')
        
        print(f'[WFOLIO] Found {len(pieces)} piece elements')
        
        for piece in pieces:
            try:
                piece_id = piece.get('data-piece-id')
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É —Å –¥–∞–Ω–Ω—ã–º–∏ –≥–∞–ª–µ—Ä–µ–∏
                link = piece.find('a', attrs={'data-gallery-title': True})
                
                if link:
                    title = link.get('data-gallery-title', '').strip()
                    
                    # –ò—â–µ–º img —Ç–µ–≥ –≤–Ω—É—Ç—Ä–∏ piece
                    img = piece.find('img', class_='lazyload')
                    
                    if img:
                        # –ü–æ–ª—É—á–∞–µ–º srcset —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è–º–∏
                        srcset = img.get('data-srcset', '')
                        
                        if srcset:
                            # srcset —Ñ–æ—Ä–º–∞—Ç–∞: "url1 1280w, url2 1920w, url3 2560w"
                            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π (—Å–∞–º—ã–π –±–æ–ª—å—à–æ–π)
                            srcset_parts = [s.strip() for s in srcset.split(',')]
                            
                            if srcset_parts:
                                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å (—Å–∞–º–æ–µ –±–æ–ª—å—à–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ)
                                largest = srcset_parts[-1].split()[0]
                                
                                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                                if largest.startswith('//'):
                                    img_url = f"{parsed.scheme}:{largest}"
                                else:
                                    img_url = urljoin(url, largest)
                                
                                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
                                if title:
                                    filename = title
                                elif piece_id:
                                    filename = f'photo_{piece_id}.jpg'
                                else:
                                    filename = os.path.basename(urlparse(img_url).path.split('?')[0])
                                
                                # –û—á–∏—â–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                                filename = filename.strip()
                                if not filename:
                                    filename = f'image_{len(files)+1}.jpg'
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
                                if not any(filename.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                                    filename += '.jpg'
                                
                                if img_url not in seen_urls:
                                    seen_urls.add(img_url)
                                    files.append({
                                        'url': img_url,
                                        'name': filename
                                    })
                                    print(f'[WFOLIO] Found image: {filename} -> {img_url[:80]}...')
            
            except Exception as e:
                print(f'[WFOLIO] Error processing piece: {str(e)}')
                continue
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ data-gallery-versions, –ø—Ä–æ–±—É–µ–º –∏—Å–∫–∞—Ç—å –æ–±—ã—á–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º
        if not files:
            print('[WFOLIO] No images found via data-gallery-versions, trying img tags')
            
            for img in soup.find_all('img'):
                src = img.get('src') or img.get('data-src')
                srcset = img.get('srcset', '')
                
                # –ò–∑ srcset –±–µ—Ä–µ–º —Å–∞–º–æ–µ –±–æ–ª—å—à–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                if srcset:
                    srcset_parts = [s.strip().split()[0] for s in srcset.split(',')]
                    if srcset_parts:
                        src = srcset_parts[-1]  # –ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±—ã—á–Ω–æ —Å–∞–º–æ–µ –±–æ–ª—å—à–æ–µ
                
                if src:
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π URL
                    if src.startswith('//'):
                        src = f"{parsed.scheme}:{src}"
                    elif not src.startswith('http'):
                        src = urljoin(url, src)
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    if any(skip in src.lower() for skip in ['icon', 'logo', 'avatar', 'sprite', 'placeholder']):
                        continue
                    
                    if src not in seen_urls:
                        filename = os.path.basename(urlparse(src).path.split('?')[0])
                        if filename and any(filename.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                            seen_urls.add(src)
                            files.append({
                                'url': src,
                                'name': filename
                            })
        
        print(f'[WFOLIO] Found {len(files)} images total')
        
        return files
    
    except Exception as e:
        print(f'[WFOLIO] Error: {str(e)}')
        raise ValueError(f'–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ wfolio –≥–∞–ª–µ—Ä–µ–∏: {str(e)}')